nohup: ignoring input
/home/yangz2/anaconda3/envs/quan_cl/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yangz2/anaconda3/envs/quan_cl/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Files already downloaded and verified
Files already downloaded and verified
[Epoch 1, Batch 100] loss: 4.771
[Epoch 1, Batch 200] loss: 4.731
[Epoch 1, Batch 300] loss: 4.685
[Epoch 1, Batch 400] loss: 4.654
[Epoch 1, Batch 500] loss: 4.643
[Epoch 1, Batch 600] loss: 4.627
[Epoch 1, Batch 700] loss: 4.628
[Epoch 1, Batch 800] loss: 4.621
[Epoch 1, Batch 900] loss: 4.619
[Epoch 1, Batch 1000] loss: 4.621
[Epoch 1, Batch 1100] loss: 4.619
[Epoch 1, Batch 1200] loss: 4.619
[Epoch 1, Batch 1300] loss: 4.617
[Epoch 1, Batch 1400] loss: 4.619
[Epoch 1, Batch 1500] loss: 4.618
[Epoch 2, Batch 100] loss: 4.617
[Epoch 2, Batch 200] loss: 4.618
[Epoch 2, Batch 300] loss: 4.617
[Epoch 2, Batch 400] loss: 4.617
[Epoch 2, Batch 500] loss: 4.619
[Epoch 2, Batch 600] loss: 4.617
[Epoch 2, Batch 700] loss: 4.616
[Epoch 2, Batch 800] loss: 4.617
[Epoch 2, Batch 900] loss: 4.619
[Epoch 2, Batch 1000] loss: 4.617
[Epoch 2, Batch 1100] loss: 4.617
[Epoch 2, Batch 1200] loss: 4.618
[Epoch 2, Batch 1300] loss: 4.617
[Epoch 2, Batch 1400] loss: 4.618
[Epoch 2, Batch 1500] loss: 4.617
[Epoch 3, Batch 100] loss: 4.616
[Epoch 3, Batch 200] loss: 4.617
[Epoch 3, Batch 300] loss: 4.618
[Epoch 3, Batch 400] loss: 4.617
[Epoch 3, Batch 500] loss: 4.618
[Epoch 3, Batch 600] loss: 4.617
[Epoch 3, Batch 700] loss: 4.616
[Epoch 3, Batch 800] loss: 4.618
[Epoch 3, Batch 900] loss: 4.618
[Epoch 3, Batch 1000] loss: 4.618
[Epoch 3, Batch 1100] loss: 4.616
[Epoch 3, Batch 1200] loss: 4.619
[Epoch 3, Batch 1300] loss: 4.618
[Epoch 3, Batch 1400] loss: 4.618
[Epoch 3, Batch 1500] loss: 4.617
[Epoch 4, Batch 100] loss: 4.617
[Epoch 4, Batch 200] loss: 4.618
[Epoch 4, Batch 300] loss: 4.616
[Epoch 4, Batch 400] loss: 4.616
[Epoch 4, Batch 500] loss: 4.618
[Epoch 4, Batch 600] loss: 4.617
[Epoch 4, Batch 700] loss: 4.618
[Epoch 4, Batch 800] loss: 4.617
[Epoch 4, Batch 900] loss: 4.619
[Epoch 4, Batch 1000] loss: 4.618
[Epoch 4, Batch 1100] loss: 4.618
[Epoch 4, Batch 1200] loss: 4.618
[Epoch 4, Batch 1300] loss: 4.617
[Epoch 4, Batch 1400] loss: 4.618
[Epoch 4, Batch 1500] loss: 4.618
[Epoch 5, Batch 100] loss: 4.618
[Epoch 5, Batch 200] loss: 4.616
[Epoch 5, Batch 300] loss: 4.614
[Epoch 5, Batch 400] loss: 4.618
[Epoch 5, Batch 500] loss: 4.619
[Epoch 5, Batch 600] loss: 4.618
[Epoch 5, Batch 700] loss: 4.618
[Epoch 5, Batch 800] loss: 4.618
[Epoch 5, Batch 900] loss: 4.617
[Epoch 5, Batch 1000] loss: 4.618
[Epoch 5, Batch 1100] loss: 4.617
[Epoch 5, Batch 1200] loss: 4.616
[Epoch 5, Batch 1300] loss: 4.619
[Epoch 5, Batch 1400] loss: 4.618
[Epoch 5, Batch 1500] loss: 4.617
[Epoch 6, Batch 100] loss: 4.617
[Epoch 6, Batch 200] loss: 4.617
[Epoch 6, Batch 300] loss: 4.618
[Epoch 6, Batch 400] loss: 4.617
[Epoch 6, Batch 500] loss: 4.617
[Epoch 6, Batch 600] loss: 4.617
[Epoch 6, Batch 700] loss: 4.619
[Epoch 6, Batch 800] loss: 4.618
[Epoch 6, Batch 900] loss: 4.619
[Epoch 6, Batch 1000] loss: 4.617
[Epoch 6, Batch 1100] loss: 4.619
[Epoch 6, Batch 1200] loss: 4.618
[Epoch 6, Batch 1300] loss: 4.618
[Epoch 6, Batch 1400] loss: 4.616
[Epoch 6, Batch 1500] loss: 4.617
[Epoch 7, Batch 100] loss: 4.617
[Epoch 7, Batch 200] loss: 4.617
[Epoch 7, Batch 300] loss: 4.617
[Epoch 7, Batch 400] loss: 4.618
[Epoch 7, Batch 500] loss: 4.615
[Epoch 7, Batch 600] loss: 4.618
[Epoch 7, Batch 700] loss: 4.617
[Epoch 7, Batch 800] loss: 4.618
[Epoch 7, Batch 900] loss: 4.618
[Epoch 7, Batch 1000] loss: 4.617
[Epoch 7, Batch 1100] loss: 4.619
[Epoch 7, Batch 1200] loss: 4.617
[Epoch 7, Batch 1300] loss: 4.618
[Epoch 7, Batch 1400] loss: 4.617
[Epoch 7, Batch 1500] loss: 4.618
[Epoch 8, Batch 100] loss: 4.618
[Epoch 8, Batch 200] loss: 4.618
[Epoch 8, Batch 300] loss: 4.618
[Epoch 8, Batch 400] loss: 4.618
[Epoch 8, Batch 500] loss: 4.616
[Epoch 8, Batch 600] loss: 4.617
[Epoch 8, Batch 700] loss: 4.618
[Epoch 8, Batch 800] loss: 4.618
[Epoch 8, Batch 900] loss: 4.617
[Epoch 8, Batch 1000] loss: 4.619
[Epoch 8, Batch 1100] loss: 4.618
[Epoch 8, Batch 1200] loss: 4.617
[Epoch 8, Batch 1300] loss: 4.618
[Epoch 8, Batch 1400] loss: 4.617
[Epoch 8, Batch 1500] loss: 4.617
[Epoch 9, Batch 100] loss: 4.615
[Epoch 9, Batch 200] loss: 4.615
[Epoch 9, Batch 300] loss: 4.618
[Epoch 9, Batch 400] loss: 4.618
[Epoch 9, Batch 500] loss: 4.617
[Epoch 9, Batch 600] loss: 4.618
[Epoch 9, Batch 700] loss: 4.617
[Epoch 9, Batch 800] loss: 4.620
[Epoch 9, Batch 900] loss: 4.617
[Epoch 9, Batch 1000] loss: 4.618
[Epoch 9, Batch 1100] loss: 4.619
[Epoch 9, Batch 1200] loss: 4.617
[Epoch 9, Batch 1300] loss: 4.618
[Epoch 9, Batch 1400] loss: 4.617
[Epoch 9, Batch 1500] loss: 4.618
[Epoch 10, Batch 100] loss: 4.618
[Epoch 10, Batch 200] loss: 4.617
[Epoch 10, Batch 300] loss: 4.618
[Epoch 10, Batch 400] loss: 4.618
[Epoch 10, Batch 500] loss: 4.618
[Epoch 10, Batch 600] loss: 4.617
[Epoch 10, Batch 700] loss: 4.618
[Epoch 10, Batch 800] loss: 4.618
[Epoch 10, Batch 900] loss: 4.618
[Epoch 10, Batch 1000] loss: 4.617
[Epoch 10, Batch 1100] loss: 4.618
[Epoch 10, Batch 1200] loss: 4.617
[Epoch 10, Batch 1300] loss: 4.617
[Epoch 10, Batch 1400] loss: 4.617
[Epoch 10, Batch 1500] loss: 4.618
Finished Training
Accuracy of the student model on the CIFAR-100 test images: 1.0%
