#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# import torch
# import torch.nn as nn
# import torch.optim as optim
# import matplotlib.pyplot as plt
# from avalanche.benchmarks import SplitMNIST
# from avalanche.models import SimpleMLP
# from avalanche.training import EWC
# from avalanche.training.plugins import EvaluationPlugin
# from avalanche.evaluation.metrics import accuracy_metrics
# from avalanche.logging import InteractiveLogger

# # è®¾ç½®è®¾å¤‡
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # SplitMNIST æ•°æ®é›†
# benchmark = SplitMNIST(n_experiences=5, return_task_id=False)

# # æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°
# model = SimpleMLP(num_classes=10)
# optimizer = optim.SGD(model.parameters(), lr=0.01)
# criterion = nn.CrossEntropyLoss()

# # æ—¥å¿—å™¨å’Œè¯„ä¼°å™¨
# interactive_logger = InteractiveLogger()

# eval_plugin = EvaluationPlugin(
#     accuracy_metrics(stream=True),
#     loggers=[interactive_logger]
# )

# strategy = EWC(
#     model=model,
#     optimizer=optimizer,
#     criterion=criterion,
#     ewc_lambda=0.4,
#     train_epochs=40,
#     device=device,
#     evaluator=eval_plugin
# )

# # ğŸ“Œ ç”¨äºè®°å½•æ¯ä¸ªä»»åŠ¡çš„å‡†ç¡®ç‡
# task_accuracies = []

# # å¼€å§‹è®­ç»ƒæ¯ä¸ª experience
# for experience in benchmark.train_stream:
#     print(f"\n--- Training on experience {experience.current_experience} ---")
#     strategy.train(experience)

#     print("--- Evaluating on test stream ---")
#     results = strategy.eval(benchmark.test_stream)

#     # æå–å¹¶è®°å½• accuracy
#     acc = results['Top1_Acc_Stream/eval_phase/test_stream']
#     task_accuracies.append(acc)

# # âœ… ç»˜å›¾
# plt.figure(figsize=(8, 5))
# plt.plot(range(1, len(task_accuracies) + 1), task_accuracies, marker='o')
# plt.title("EWC on SplitMNIST")
# plt.xlabel("Task (experience)")
# plt.ylabel("Test Accuracy")
# plt.grid(True)
# plt.savefig("ewc_mnist_accuracy_plot.png")
# plt.show()


# In[5]:


import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import pennylane as qml
from pennylane.qnn import TorchLayer
from torchvision import transforms

from avalanche.benchmarks.classic import SplitMNIST
from avalanche.training import EWC
from avalanche.training.plugins import EvaluationPlugin
from avalanche.evaluation.metrics import forgetting_metrics, \
accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, \
confusion_matrix_metrics, disk_usage_metrics
from avalanche.logging import InteractiveLogger
import pickle


# In[ ]:


# -----------------------------
# 1. Quantum Circuit Definition (Multi-layer)
# -----------------------------
n_qubits = 8
input_dim = 784  # 28 x 28 MNIST
n_layers = 10  # âœ… Now actually used
dev = qml.device("lightning.qubit", wires=n_qubits)

def quantum_circuit(inputs, weights):
    # 1. Amplitude embeddingï¼ˆå¿…é¡» L2 å½’ä¸€åŒ–ï¼‰
    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True)
    # weights: [n_layers, n_qubits, 2]
    for layer in range(n_layers):
        for i in range(n_qubits):
            qml.RZ(weights[layer, i, 0], wires=i)
            qml.RX(weights[layer, i, 1], wires=i)

        # Entanglement layer (odd-even alternating)
        for i in range(0, n_qubits - 1, 2):
            qml.CNOT(wires=[i, i + 1])
        for i in range(1, n_qubits - 1, 2):
            qml.CNOT(wires=[i, i + 1])

    return qml.expval(qml.PauliZ(0))

qml.drawer.use_style("pennylane")
# è¾“å…¥ï¼š8ä¸ªé‡å­æ¯”ç‰¹
inputs = torch.randn(2 ** n_qubits)

# æƒé‡ï¼š10 å±‚ï¼Œæ¯å±‚ 8 æ¯”ç‰¹ï¼Œæ¯æ¯”ç‰¹ä¸¤ä¸ªå‚æ•°ï¼ˆRZ, RXï¼‰
weights = torch.randn(n_layers, n_qubits, 2)

# ç»˜åˆ¶ç”µè·¯
fig, ax = qml.draw_mpl(quantum_circuit)(inputs, weights)
plt.show()

# -----------------------------
# 2. TorchLayer + PyTorch Model
# -----------------------------
weight_shapes = {"weights": (n_layers, n_qubits, 2)}
qnode = qml.QNode(quantum_circuit, dev, interface="torch")
qlayer = TorchLayer(qnode, weight_shapes)

class QuantumClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.q_layer = qlayer
        self.output = nn.Linear(1, 10)
        nn.init.xavier_uniform_(self.output.weight)

    def forward(self, x):
        x = self.q_layer(x)
        if len(x.shape) == 1:
            x = x.unsqueeze(1)
        x = self.output(x)
        assert not torch.isnan(x).any(), "Detected NaN in input to output layer"
        return F.log_softmax(x, dim=1)


# In[ ]:


# -----------------------------
# 3. Data Transform (Only 8 pixels used)
transform = transforms.Compose([
    transforms.Lambda(lambda x: x.view(-1)),  # å±•å¹³ä¸º 784
    transforms.Lambda(lambda x: F.pad(x, (0, 256 - 784))) if 784 < 256 else transforms.Lambda(lambda x: x[:256]),
    transforms.Lambda(lambda x: F.normalize(x, p=2, dim=0))  # L2 å½’ä¸€åŒ–
])

benchmark = SplitMNIST(n_experiences=5, return_task_id=False,
                       train_transform=transform, eval_transform=transform)

# -----------------------------
# 4. Avalanche EWC Strategy Setup
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = QuantumClassifier().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.005)
criterion = nn.NLLLoss()

interactive_logger = InteractiveLogger()
eval_plugin = EvaluationPlugin(
    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),
    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),
    timing_metrics(epoch=True, epoch_running=True),
    cpu_usage_metrics(experience=True),
    forgetting_metrics(experience=True, stream=True),
    confusion_matrix_metrics(num_classes=10, save_image=True,
                             stream=True),
    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),
    loggers=[interactive_logger]
)

strategy = EWC(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    ewc_lambda=0.4,
    train_epochs=40,
    device=device,
    evaluator=eval_plugin
)

# -----------------------------
# 5. Training & Accuracy Recording
# -----------------------------
task_accuracies = []

for experience in benchmark.train_stream:
    print(f"\n--- Training on experience {experience.current_experience} ---")
    strategy.train(experience)

    print("--- Evaluating on test stream ---")
    results = strategy.eval(benchmark.test_stream)

    acc = results['Top1_Acc_Stream/eval_phase/test_stream/Task000']
    task_accuracies.append(acc)
    

# å­˜å‚¨åˆ°æ–‡ä»¶
with open("results/list/splitminist_EWC_qbit8_qdepth10.pkl", "wb") as f:
    pickle.dump(results, f)  
    
with open("results/list/splitminist_EWC_qbit8_qdepth10.pkl", "rb") as f:
    results = pickle.load(f)  


# In[ ]:


num_experiences = 5  # ç»éªŒæ•°é‡ï¼ˆExp001 åˆ° Exp004ï¼‰
x = []  # x è½´ï¼šç»éªŒç¼–å·ï¼ˆ1-20ï¼‰
print(results)

# éå† results æå– Loss_Exp æ•°æ®
count = 1  # ç”¨äºæ ‡è®°ç»éªŒç¼–å·
loss_values = []
for train_idx, result in enumerate(results):  # éå†5æ¬¡è®­ç»ƒçš„ç»“æœ
    for exp_id in range(0, num_experiences):  # æ¯æ¬¡è®­ç»ƒå¯¹åº”4æ¬¡ evalï¼ˆExp001 åˆ° Exp004ï¼‰
        key = f"Loss_Exp/eval_phase/test_stream/Task000/Exp00{exp_id}"
        loss = result.get(key, None)
        if loss is not None:  # ç¡®ä¿é”®å­˜åœ¨
            loss_values.append(loss)
            x.append(count)  # x è½´è¿ç»­ç¼–å·
            count += 1

# éå† results æå– Top1_Acc_Exp æ•°æ®
top1_acc_values = []
for train_idx, result in enumerate(results):  # éå†5æ¬¡è®­ç»ƒçš„ç»“æœ
    for exp_id in range(0, num_experiences):  # æ¯æ¬¡è®­ç»ƒå¯¹åº”4æ¬¡ evalï¼ˆExp001 åˆ° Exp004ï¼‰
        key = f"Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp00{exp_id}"
        top1_acc = result.get(key, None)
        if top1_acc is not None:  # ç¡®ä¿é”®å­˜åœ¨
            top1_acc_values.append(top1_acc)

# éå† resutls ä¸­çš„ Forgetting æ•°æ®, ä½†æ˜¯æ˜¯æ¯ä¸ªstreamçš„Forgetting
forgetting_values = []
for train_idx, result in enumerate(results):  # éå†5æ¬¡è®­ç»ƒçš„ç»“æœ
    for exp_id in range(0, num_experiences):  # æ¯æ¬¡è®­ç»ƒå¯¹åº”4æ¬¡ evalï¼ˆExp001 åˆ° Exp004ï¼‰
        key = f"StreamForgetting/eval_phase/test_stream"
        forgetting = result.get(key, None)
        if forgetting is not None:  # ç¡®ä¿é”®å­˜åœ¨
            forgetting_values.append(forgetting)


# ç»˜åˆ¶å›¾å½¢
plt.figure(figsize=(20, 6))
plt.plot(x, loss_values, marker='o', linestyle='-', color='b', label='Loss_Exp')
plt.title("Loss over n_experience**2 Evaluations After n_experience Training Phases")
plt.xlabel("Evaluation Index")
plt.ylabel("Loss Value")
plt.xticks(range(1, len(x) + 1))  # è®¾ç½® x è½´åˆ»åº¦
plt.grid(True)
plt.legend("splitminist_EWC_qbit8_qdepth10_loss")
plt.tight_layout()
plt.savefig("results/figs/splitminist_EWC_qbit8_qdepth10_loss.png")

plt.figure(figsize=(20, 6))
plt.plot(x, top1_acc_values, marker='o', linestyle='-', color='r', label='Top1_Acc_Exp')
plt.title("Top1_Acc over n_experience**2 Evaluations After n_experience Training Phases")
plt.xlabel("Evaluation Index")
plt.ylabel("Top1_Acc Value")
plt.xticks(range(1, len(x) + 1))  # è®¾ç½® x è½´åˆ»åº¦
plt.grid(True)
plt.legend("splitminist_EWC_qbit8_qdepth10_acc")
plt.tight_layout()
plt.savefig("results/figs/splitminist_EWC_qbit8_qdepth10_acc.png")

plt.figure(figsize=(20, 6))
plt.plot(range(1, len(forgetting_values) + 1), forgetting_values, marker='o', linestyle='-', color='g', label='StreamForgetting')
plt.title("StreamForgetting over n_experience**2 Evaluations After n_experience Training Phases")
plt.xlabel("Evaluation Index")
plt.ylabel("StreamForgetting Value")
plt.xticks(range(1, len(forgetting_values) + 1))  # è®¾ç½® x è½´åˆ»åº¦
plt.grid(True)
plt.legend("splitminist_EWC_qbit8_qdepth10_forget")
plt.tight_layout()
plt.savefig("results/figs/splitminist_EWC_qbit8_qdepth10_forget.png")

