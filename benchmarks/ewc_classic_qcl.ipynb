{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d482d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 11:18:33.615345: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-16 11:18:33.653231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-16 11:18:33.664889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-16 11:18:33.688664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-16 11:18:36.632818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/yangz2/anaconda3/envs/quan_cl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 使用预处理好的 PCA 数据创建了 nc_benchmark\n",
      "Starting training...\n",
      "\n",
      "--- Training on experience 0 ---\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 11841/11841 [02:33<00:00, 77.02it/s] \n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 773.9102\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 773.9102\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0441\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0001\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 153.7251\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9830\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 11841/11841 [02:10<00:00, 91.07it/s] \n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 781.0557\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 781.0557\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0146\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 130.0138\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9954\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 11841/11841 [02:03<00:00, 95.61it/s] \n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 781.0557\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 781.0557\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0100\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 123.8484\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9973\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 1.0000\n",
      "100%|██████████| 11841/11841 [02:23<00:00, 82.24it/s] \n",
      "Epoch 3 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 781.0557\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 781.0557\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0106\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 143.9673\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9975\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 1.0000\n",
      " 85%|████████▍ | 10019/11841 [01:56<00:25, 72.12it/s]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.training import EWC\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, \\\n",
    "accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, \\\n",
    "confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 设备设置\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 普通CNN网络定义\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # 输入通道为1（灰度图像），输出通道为16，卷积核大小3x3\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)  # 假设输入是28x28，经过两次池化后为7x7\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入形状: [batch, 256] -> 需要reshape为图像格式\n",
    "        # 假设原始图像是16x16 (256=16x16)，但我们知道MNIST是28x28\n",
    "        # 这里我们reshape为28x28 (784个像素)，但我们只有256个特征\n",
    "        # 所以我们需要上采样或填充 - 这里我们使用简单的线性层进行转换\n",
    "        \n",
    "        # 如果输入是256维特征，先转换为784维\n",
    "        if x.shape[1] == 256:\n",
    "            x = x.view(-1, 1, 16, 16)  # 16x16=256\n",
    "            # 上采样到28x28\n",
    "            x = F.interpolate(x, size=(28, 28), mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            # 如果输入已经是图像格式\n",
    "            x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # 卷积层\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # 28x28 -> 14x14\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # 14x14 -> 7x7\n",
    "        \n",
    "        # 展平\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        \n",
    "        # 全连接层\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 1: 加载预处理好的 PCA 数据\n",
    "with open(\"/home/yangz2/code/quantum_cl/data/splitmnist_pca256.pkl\", 'rb') as f:\n",
    "    processed = pickle.load(f)\n",
    "\n",
    "# Step 2: 重构每个经验的 TensorDatasets（train/val）\n",
    "datasets_by_exp = {}\n",
    "for vec, label, exp_id in processed:\n",
    "    datasets_by_exp.setdefault(exp_id, {\"X\": [], \"y\": []})\n",
    "    datasets_by_exp[exp_id][\"X\"].append(vec)\n",
    "    datasets_by_exp[exp_id][\"y\"].append(label)\n",
    "\n",
    "# Step 3: 将每个 experience 的数据打包为 TensorDatasets\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "task_labels = []\n",
    "for exp_id in sorted(datasets_by_exp):\n",
    "    X = torch.stack([torch.tensor(v) for v in datasets_by_exp[exp_id][\"X\"]])\n",
    "    y = torch.tensor(datasets_by_exp[exp_id][\"y\"], dtype=torch.long)\n",
    "    \n",
    "    ds = TensorDataset(X, y)\n",
    "    \n",
    "    train_datasets.append(ds)\n",
    "    test_datasets.append(ds)\n",
    "    task_labels.append(0)\n",
    "\n",
    "# Step 4: 使用 nc_benchmark 创建持续学习基准\n",
    "benchmark = nc_benchmark(\n",
    "    train_datasets, \n",
    "    test_datasets, \n",
    "    n_experiences=len(train_datasets),\n",
    "    task_labels=task_labels\n",
    ")\n",
    "\n",
    "print(\"✔ 使用预处理好的 PCA 数据创建了 nc_benchmark\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 设置训练环境\n",
    "# -----------------------------------------------------------------------------\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluation setup\n",
    "interactive_logger = InteractiveLogger()\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=10, save_image=False, stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger]\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 使用 EWC 持续学习策略\n",
    "# -----------------------------------------------------------------------------\n",
    "strategy = EWC(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    ewc_lambda=1,\n",
    "    train_epochs=10,\n",
    "    device=device,\n",
    "    evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 开始训练与评估\n",
    "# -----------------------------------------------------------------------------\n",
    "task_accuracies = []\n",
    "save_dir = \"/home/yangz2/code/quantum_cl/results/list\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for experience in benchmark.train_stream:\n",
    "    print(f\"\\n--- Training on experience {experience.current_experience} ---\")\n",
    "    strategy.train(experience)\n",
    "    \n",
    "    print(f\"--- Evaluating after experience {experience.current_experience} ---\")\n",
    "    results = strategy.eval(benchmark.test_stream)\n",
    "    task_accuracies.append(results)\n",
    "    \n",
    "    # 保存中间结果\n",
    "    with open(os.path.join(save_dir, f\"splitmnist_EWC_classic_cnn_interim_results_exp_{experience.current_experience}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(task_accuracies, f)\n",
    "\n",
    "# 保存最终结果\n",
    "with open(os.path.join(save_dir, \"splitmnist_EWC_classic_cnn_final.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(task_accuracies, f)\n",
    "\n",
    "print(\"✔ Training and evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa38e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quan_cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
