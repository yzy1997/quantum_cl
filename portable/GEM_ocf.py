#!/usr/bin/env python
# coding: utf-8

# In[8]:


import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import pennylane as qml
from pennylane.qnn import TorchLayer
from torchvision import transforms
from torch.optim.lr_scheduler import StepLR

from avalanche.benchmarks.classic import SplitMNIST
from avalanche.training import Naive
from avalanche.training.plugins import GEMPlugin, EWCPlugin, EvaluationPlugin, LRSchedulerPlugin
from avalanche.evaluation.metrics import forgetting_metrics, \
accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, \
confusion_matrix_metrics, disk_usage_metrics
from avalanche.logging import InteractiveLogger
import pickle
import os


# In[2]:


# -----------------------------
# 1. Quantum Circuit Definition (Multi-layer)
# -----------------------------
n_qubits = 10
input_dim = 2**n_qubits  # 28 x 28 MNIST
n_layers = 10  # âœ… Now actually used
dev = qml.device("lightning.qubit", wires=n_qubits)
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

def quantum_circuit(inputs, weights):
    # ğŸ’¡ Clamp é™åˆ¶è¾“å…¥èŒƒå›´ï¼Œé˜²æ­¢æç«¯å½’ä¸€åŒ–åæŒ¯å¹…å¾ˆå°æˆ–å¾ˆå¤§
    inputs = qml.math.clip(inputs, -1.0, 1.0)
    # 1. Amplitude embeddingï¼ˆå¿…é¡» L2 å½’ä¸€åŒ–ï¼‰
    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True, pad_with=0.0)
    # weights: [n_layers, n_qubits, 2]
    for layer in range(n_layers):
        for i in range(n_qubits):
            qml.RX(weights[layer, i, 1], wires=i)
            qml.RZ(weights[layer, i, 0], wires=i)

        # Entanglement layer (odd-even alternating)
        for i in range(1, n_qubits - 1, 2):
            qml.CNOT(wires=[i, i + 1])
        for i in range(0, n_qubits - 1, 2):
            qml.CNOT(wires=[i, i + 1])
        for i in range(1, n_qubits - 1, 2):
            qml.CNOT(wires=[i, i + 1])
        for i in range(0, n_qubits - 1, 2):
            qml.CNOT(wires=[i, i + 1])
        for i in range(n_qubits):
            qml.RZ(weights[layer, i, 0], wires=i)
            qml.RX(weights[layer, i, 1], wires=i)

    return qml.expval(qml.PauliZ(0))

qml.drawer.use_style("pennylane")
# è¾“å…¥ï¼š8ä¸ªé‡å­æ¯”ç‰¹
inputs = torch.randn(2 ** n_qubits)

# æƒé‡ï¼š10 å±‚ï¼Œæ¯å±‚ 8 æ¯”ç‰¹ï¼Œæ¯æ¯”ç‰¹ä¸¤ä¸ªå‚æ•°ï¼ˆRZ, RXï¼‰
weights = torch.randn(n_layers, n_qubits, 2)

# ç»˜åˆ¶ç”µè·¯
fig, ax = qml.draw_mpl(quantum_circuit)(inputs, weights)
plt.show()


# In[3]:


# -----------------------------
# 2. TorchLayer + PyTorch Model
# -----------------------------

class QuantumClassifier(nn.Module):
    def __init__(self, device):
        super().__init__()
        self.device = device

        weight_shapes = {"weights": (n_layers, n_qubits, 2)}
        qnode = qml.QNode(quantum_circuit, dev, interface="torch")
        # TorchLayer è¿”å›çš„å¼ é‡é»˜è®¤åœ¨ CPU ä¸Š
        self.q_layer = TorchLayer(qnode, weight_shapes)
        # ç»å…¸å±‚æ¬åˆ°ä½ é€‰çš„ deviceï¼ˆæ¯”å¦‚ cuda:1ï¼‰
        self.output = nn.Linear(1, 10).to(self.device)

    def forward(self, x):
        x = x.to(self.device)              # æŠŠè¾“å…¥ä¸¢ç»™ GPUï¼ˆæˆ– CPUï¼‰
        x = torch.nan_to_num(x, 0.0, 1.0, -1.0)
        x = F.normalize(x + 1e-8, p=2, dim=1)

        # é‡å­å±‚æ¨¡æ‹Ÿä¸€å®šæ˜¯åœ¨ CPU ä¸Šï¼Œç»“æœä¹Ÿæ˜¯ CPU tensor
        x = self.q_layer(x)
        # è¿™é‡ŒæŠŠå®ƒæ¬åˆ° self.device
        x = x.to(self.device)

        x = x.unsqueeze(1)
        x = self.output(x)
        return F.log_softmax(x, dim=1)


# In[4]:


from torchvision import transforms

mnist_transform = transforms.Compose([
    transforms.Resize((32, 32)),      # 28Ã—28 â†’ 32Ã—32
    # transforms.ToTensor(),            # [0,1]ï¼Œshape=[1,32,32]
    transforms.Lambda(lambda x: x.view(-1))  # flatten åˆ° (1024,)
])

benchmark = SplitMNIST(
    n_experiences=5,
    return_task_id=False,
    train_transform=mnist_transform,
    eval_transform=mnist_transform
)


# In[5]:


# -----------------------------
# 4. Avalanche EWC Strategy Setup
# -----------------------------

model = QuantumClassifier(device).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.NLLLoss().to(device)

interactive_logger = InteractiveLogger()
eval_plugin = EvaluationPlugin(
    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),
    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),
    timing_metrics(epoch=True, epoch_running=True),
    cpu_usage_metrics(experience=True),
    forgetting_metrics(experience=True, stream=True),
    confusion_matrix_metrics(num_classes=10, save_image=True,
                             stream=True),
    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),
    loggers=[interactive_logger]
)


# In[9]:


# 6. é…ç½®GEMç­–ç•¥ï¼ˆè°ƒä¼˜å‚æ•°ï¼‰
gem_plugin = GEMPlugin(
    patterns_per_experience=300,  # æ¯ä¸ªä»»åŠ¡ä¿ç•™300ä¸ªæ ·æœ¬
    memory_strength=0.3          # é™ä½çº¦æŸå¼ºåº¦ï¼ˆæ›´å®½æ¾ï¼‰
)

# 7. ä½¿ç”¨åœ¨çº¿æ¨¡å¼çš„EWCæ­£åˆ™åŒ–æ’ä»¶
ewc_plugin = EWCPlugin(
    ewc_lambda=0.4,              # EWCæ­£åˆ™åŒ–å¼ºåº¦
    mode="online",               # å¿…é¡»è®¾ç½®ä¸ºonlineæ‰èƒ½ä½¿ç”¨decay_factor
    decay_factor=0.9,            # FisherçŸ©é˜µè¡°å‡ç³»æ•°
    keep_importance_data=True    # ä¿ç•™é‡è¦æ€§æ•°æ®ç”¨äºåç»­ä»»åŠ¡
)

# 8. æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.7)
lr_plugin = LRSchedulerPlugin(lr_scheduler)

# è®­ç»ƒç­–ç•¥
strategy = Naive(
    model,
    optimizer,
    criterion,
    train_mb_size=128,           # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    eval_mb_size=256,            # è¯„ä¼°æ‰¹æ¬¡å¤§å°
    train_epochs=15,             # å¢åŠ è®­ç»ƒè½®æ•°
    plugins=[gem_plugin, ewc_plugin, lr_plugin],
    evaluator=eval_plugin,
    device=device,
    # eval_every=1                 # æ¯ä¸ªepochåéƒ½è¯„ä¼°
)

# æ—¥å¿—è®°å½•
interactive_logger = InteractiveLogger()
# text_logger = TextLogger(open('ewc_splitmnist.log', 'w'))
# tb_logger = TensorboardLogger('ewc_tb_logs')
strategy.evaluator.loggers = [interactive_logger]


# In[10]:


# -----------------------------
# 5. Training & Accuracy Recording
# -----------------------------
task_accuracies = []

for experience in benchmark.train_stream:
    print(f"\n--- Training on experience {experience.current_experience} ---")
    strategy.train(experience)

    print("--- Evaluating on test stream ---")
    results = strategy.eval(benchmark.test_stream)

    task_accuracies.append(results)


# In[ ]:


# Define the file path
file_path = "/home/yangz2/code/quantum_cl/results/list/splitminist_GEM_ocf_qbit10_qdepth10.pkl"

# Create directories if they don't exist
os.makedirs(os.path.dirname(file_path), exist_ok=True)  # <-- Add this line   


# In[ ]:


# å­˜å‚¨åˆ°æ–‡ä»¶
with open(file_path, "wb") as f:
    pickle.dump([task_accuracies], f)  

